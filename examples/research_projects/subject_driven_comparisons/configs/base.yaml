pretrained_model_name_or_path: runwayml/stable-diffusion-v1-5
revision: # "Revision of pretrained model identifier from huggingface.co/models.

# The name of the Dataset (from the FF hub) to train on (could be your own private,
# dataset). It can also be a path pointing to a local copy of a dataset in your
# filesystem, or to a folder containing files that HF datasets can understand.
dataset_name: lambdalabs/pokemon-blip-captions
# The config of the Dataset, leave as None if there's only one config
dataset_config_name:
# A folder containing the training data. Folder contents must follow the structure
# described in https://huggingface.co/docs/datasets/image_dataset#imagefolder. In
# particular, a `metadata.jsonl` file" must exist to provide the captions for the
# images. Ignored if `dataset_name` is specified."
train_data_dir:
# The column of the dataset containing an image.
image_column: image
# The column of the dataset containing a caption or a list of captions.
caption_column: text
# A prompt that is sampled during training for inference.
validation_prompt:
# Number of images that should be generated during validation with `validation_prompt`.
num_validation_images: 4
# Run fine-tuning validation every `validation_epochs` epochs.
# The validation process consists of running the prompt
validation_epochs: 1
# For debugging/quicker training, truncate the number of training examples (if set)
max_train_samples:
# The output directory where the model predictions and checkpoints will be written.
output_dir: sd-model-finetuned-lora
# The directory where the downloaded models and datasets will be stored.
cache_dir:
# A seed for reproducible training.
seed:
# The resolution for input images, all the images in the train/validation dataset will be resized to this resolution
resolution: 512
# Whether to center crop the input images to the resolution. If not set, the images
# will be randomly cropped.
# The images will be resized to the resolution first before cropping!
center_crop: False
# Whether to randomly flip the input images horizontally.
random_flip: False
# Batch size (per device) for the training dataloader.
train_batch_size: 4
# Number of training epochs.
num_train_epochs: 100
# Total number of training steps to perform.  If provided, overrides num_train_epochs.
max_train_steps:
# Number of updates steps to accumulate before performing a backward/update pass
gradient_accumulation_steps: 1
# Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.
gradient_checkpointing: False
# Initial learning rate (after the potential warmup period) to use.
learning_rate: 1e-4
# Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.
scale_lr: False
# The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
lr_scheduler: constant
# Number of steps for the warmup in the lr scheduler
# TODO <- scheduler to get its own config and warmup steps goes in those who have it
lr_warmup_steps: 500
# SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0.
# More details here: https://arxiv.org/abs/2303.09556
snr_gamma:
# Whether or not to use 8-bit Adam from bitsandbytes.
use_8bit_adam: False
# Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. See
# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
allow_tf32: False
# Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.
dataloader_num_workers: 0

# The beta1 parameter for the Adam optimizer. # TODO this goes in the optimizer config
adam_beta1: 0.9
# The beta2 parameter for the Adam optimizer. # TODO this goes in the optimizer config
adam_beta2: 0.999
# Weight decay to use. # TODO this goes in the optimizer config
adam_weight_decay: 1e-2
# Epsilon value for the Adam optimizer. # TODO this goes in the optimizer config
adam_epsilon: 1e-08
# Max gradient norm.
max_grad_norm: 1.0
# Whether or not to push the model to the Hub.
push_to_hub: False
# The token to use to push to the Model Hub.
hub_token:
# The prediction_type that shall be used for training.
# Choose between 'epsilon' or 'v_prediction' or leave `None`. If left to `None` the
# default prediction type will be `noise_scheduler.config.prediciton_type`.
prediction_type:
# The name of the repository to keep in sync with the local `output_dir
hub_model_id:
# [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to
# *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.
logging_dir: logs
# Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16
# requires PyTorch>=1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate
# config of the current system or the flag passed with the `accelerate.launch` command.
# Use this argument to override the accelerate config.
mixed_precision: # TODO this goes in the accelerator config
# The integration to report the results and logs to. Supported platforms are
# `tensorboard` (default), `wandb` and `comet_ml`. Use `all` to report to all
report_to: wandb
# For distributed training: local_rank
local_rank: -1
# Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming training using `--resume_from_checkpoint`
checkpointing_steps: 500
# Max number of checkpoints to store. Passed as `total_limit` to the `Accelerator` `ProjectConfiguration`. See Accelerator::save_state https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.save_state for more docs
checkpoints_total_limit: 2
# Whether training should be resumed from a previous checkpoint. Use a path saved by `--checkpointing_steps`, or `latest` to automatically select the last available checkpoint.
resume_from_checkpoint:
# Whether or not to use xformers.
enable_xformers_memory_efficient_attention: False
# The scale of noise offset.
noise_offset: 0.0
